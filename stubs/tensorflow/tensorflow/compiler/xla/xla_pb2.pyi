"""
@generated by mypy-protobuf.  Do not edit manually!
isort:skip_file
Copyright 2017 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"""
import builtins
import collections.abc
import google.protobuf.descriptor
import google.protobuf.internal.containers
import google.protobuf.internal.enum_type_wrapper
import google.protobuf.message
import sys
import tensorflow.compiler.xla.service.hlo_pb2
import tensorflow.compiler.xla.xla_data_pb2
import typing

if sys.version_info >= (3, 10):
    import typing as typing_extensions
else:
    import typing_extensions

DESCRIPTOR: google.protobuf.descriptor.FileDescriptor

@typing_extensions.final
class DebugOptions(google.protobuf.message.Message):
    """Debugging options for XLA. These options may change at any time - there are
    no guarantees about backward or forward compatibility for these fields.
    """

    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    class _ShapeChecks:
        ValueType = typing.NewType("ValueType", builtins.int)
        V: typing_extensions.TypeAlias = ValueType

    class _ShapeChecksEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[DebugOptions._ShapeChecks.ValueType], builtins.type):  # noqa: F821
        DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor
        IGNORE: DebugOptions._ShapeChecks.ValueType  # 0
        """Do not insert any shape checks for dynamically shaped operations; output
        buffers might contain garbage data if shapes don't match.
        """
        RUNTIME: DebugOptions._ShapeChecks.ValueType  # 1
        """Check shapes at runtime, will insert an extra synchronization if shapes
        cannot be proven correct at compile time.
        """
        COMPILE_TIME: DebugOptions._ShapeChecks.ValueType  # 2
        """Will refuse to compile any program where shape correctness can not be
        established at compile time.
        """

    class ShapeChecks(_ShapeChecks, metaclass=_ShapeChecksEnumTypeWrapper): ...
    IGNORE: DebugOptions.ShapeChecks.ValueType  # 0
    """Do not insert any shape checks for dynamically shaped operations; output
    buffers might contain garbage data if shapes don't match.
    """
    RUNTIME: DebugOptions.ShapeChecks.ValueType  # 1
    """Check shapes at runtime, will insert an extra synchronization if shapes
    cannot be proven correct at compile time.
    """
    COMPILE_TIME: DebugOptions.ShapeChecks.ValueType  # 2
    """Will refuse to compile any program where shape correctness can not be
    established at compile time.
    """

    class _StepMarkerLocation:
        ValueType = typing.NewType("ValueType", builtins.int)
        V: typing_extensions.TypeAlias = ValueType

    class _StepMarkerLocationEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[DebugOptions._StepMarkerLocation.ValueType], builtins.type):  # noqa: F821
        DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor
        STEP_MARK_AT_ENTRY: DebugOptions._StepMarkerLocation.ValueType  # 0
        """Generate a step marker at the program entry. This handles the case where
        each step is done by one or multiple program execution(s). Only the first
        program will be tagged for generating a step marker at the program entry.
        This is the default.
        """
        STEP_MARK_AT_TOP_LEVEL_WHILE_LOOP: DebugOptions._StepMarkerLocation.ValueType  # 1
        """Generate a step marker at each iteration of the top level while loop,
        which is assumed to be a training loop.
        """
        STEP_MARK_AT_SECOND_LEVEL_WHILE_LOOP: DebugOptions._StepMarkerLocation.ValueType  # 3
        """Generate a step marker at each iteration of the second level while loops,
        which is assumed to be a training or eval loop.
        """
        STEP_MARK_NONE: DebugOptions._StepMarkerLocation.ValueType  # 2
        """No step marker generated."""

    class StepMarkerLocation(_StepMarkerLocation, metaclass=_StepMarkerLocationEnumTypeWrapper): ...
    STEP_MARK_AT_ENTRY: DebugOptions.StepMarkerLocation.ValueType  # 0
    """Generate a step marker at the program entry. This handles the case where
    each step is done by one or multiple program execution(s). Only the first
    program will be tagged for generating a step marker at the program entry.
    This is the default.
    """
    STEP_MARK_AT_TOP_LEVEL_WHILE_LOOP: DebugOptions.StepMarkerLocation.ValueType  # 1
    """Generate a step marker at each iteration of the top level while loop,
    which is assumed to be a training loop.
    """
    STEP_MARK_AT_SECOND_LEVEL_WHILE_LOOP: DebugOptions.StepMarkerLocation.ValueType  # 3
    """Generate a step marker at each iteration of the second level while loops,
    which is assumed to be a training or eval loop.
    """
    STEP_MARK_NONE: DebugOptions.StepMarkerLocation.ValueType  # 2
    """No step marker generated."""

    @typing_extensions.final
    class XlaBackendExtraOptionsEntry(google.protobuf.message.Message):
        DESCRIPTOR: google.protobuf.descriptor.Descriptor

        KEY_FIELD_NUMBER: builtins.int
        VALUE_FIELD_NUMBER: builtins.int
        key: builtins.str
        value: builtins.str
        def __init__(
            self,
            *,
            key: builtins.str | None = ...,
            value: builtins.str | None = ...,
        ) -> None: ...
        def ClearField(self, field_name: typing_extensions.Literal["key", b"key", "value", b"value"]) -> None: ...

    XLA_HLO_GRAPH_ADDRESSES_FIELD_NUMBER: builtins.int
    XLA_HLO_PROFILE_FIELD_NUMBER: builtins.int
    XLA_DISABLE_HLO_PASSES_FIELD_NUMBER: builtins.int
    XLA_ENABLE_HLO_PASSES_ONLY_FIELD_NUMBER: builtins.int
    XLA_DISABLE_ALL_HLO_PASSES_FIELD_NUMBER: builtins.int
    XLA_BACKEND_OPTIMIZATION_LEVEL_FIELD_NUMBER: builtins.int
    XLA_EMBED_IR_IN_EXECUTABLE_FIELD_NUMBER: builtins.int
    XLA_ELIMINATE_HLO_IMPLICIT_BROADCAST_FIELD_NUMBER: builtins.int
    XLA_CPU_MULTI_THREAD_EIGEN_FIELD_NUMBER: builtins.int
    XLA_GPU_CUDA_DATA_DIR_FIELD_NUMBER: builtins.int
    XLA_GPU_FTZ_FIELD_NUMBER: builtins.int
    XLA_LLVM_ENABLE_ALIAS_SCOPE_METADATA_FIELD_NUMBER: builtins.int
    XLA_LLVM_ENABLE_NOALIAS_METADATA_FIELD_NUMBER: builtins.int
    XLA_LLVM_ENABLE_INVARIANT_LOAD_METADATA_FIELD_NUMBER: builtins.int
    XLA_LLVM_DISABLE_EXPENSIVE_PASSES_FIELD_NUMBER: builtins.int
    XLA_TEST_ALL_OUTPUT_LAYOUTS_FIELD_NUMBER: builtins.int
    XLA_TEST_ALL_INPUT_LAYOUTS_FIELD_NUMBER: builtins.int
    XLA_HLO_GRAPH_SHARDING_COLOR_FIELD_NUMBER: builtins.int
    XLA_CPU_USE_MKL_DNN_FIELD_NUMBER: builtins.int
    XLA_CPU_USE_XLA_RUNTIME_FIELD_NUMBER: builtins.int
    XLA_GPU_MAX_KERNEL_UNROLL_FACTOR_FIELD_NUMBER: builtins.int
    XLA_CPU_ENABLE_FAST_MATH_FIELD_NUMBER: builtins.int
    XLA_CPU_FAST_MATH_HONOR_NANS_FIELD_NUMBER: builtins.int
    XLA_CPU_FAST_MATH_HONOR_INFS_FIELD_NUMBER: builtins.int
    XLA_CPU_FAST_MATH_HONOR_DIVISION_FIELD_NUMBER: builtins.int
    XLA_CPU_FAST_MATH_HONOR_FUNCTIONS_FIELD_NUMBER: builtins.int
    XLA_CPU_ENABLE_FAST_MIN_MAX_FIELD_NUMBER: builtins.int
    XLA_GPU_ENABLE_FAST_MIN_MAX_FIELD_NUMBER: builtins.int
    XLA_ALLOW_EXCESS_PRECISION_FIELD_NUMBER: builtins.int
    XLA_GPU_CRASH_ON_VERIFICATION_FAILURES_FIELD_NUMBER: builtins.int
    XLA_GPU_AUTOTUNE_LEVEL_FIELD_NUMBER: builtins.int
    XLA_FORCE_HOST_PLATFORM_DEVICE_COUNT_FIELD_NUMBER: builtins.int
    XLA_GPU_DISABLE_GPUASM_OPTIMIZATIONS_FIELD_NUMBER: builtins.int
    XLA_GPU_SHAPE_CHECKS_FIELD_NUMBER: builtins.int
    XLA_CPU_ENABLE_MLIR_LOWERING_FIELD_NUMBER: builtins.int
    XLA_GPU_ENABLE_MLIR_LOWERING_FIELD_NUMBER: builtins.int
    XLA_HLO_EVALUATOR_USE_FAST_PATH_FIELD_NUMBER: builtins.int
    XLA_ALLOW_SCALAR_INDEX_DYNAMIC_OPS_FIELD_NUMBER: builtins.int
    XLA_STEP_MARKER_LOCATION_FIELD_NUMBER: builtins.int
    XLA_DUMP_TO_FIELD_NUMBER: builtins.int
    XLA_DUMP_HLO_MODULE_RE_FIELD_NUMBER: builtins.int
    XLA_DUMP_HLO_PASS_RE_FIELD_NUMBER: builtins.int
    XLA_DUMP_HLO_AS_TEXT_FIELD_NUMBER: builtins.int
    XLA_DUMP_HLO_AS_PROTO_FIELD_NUMBER: builtins.int
    XLA_DUMP_HLO_AS_DOT_FIELD_NUMBER: builtins.int
    XLA_DUMP_HLO_AS_URL_FIELD_NUMBER: builtins.int
    XLA_DUMP_HLO_AS_HTML_FIELD_NUMBER: builtins.int
    XLA_DUMP_FUSION_VISUALIZATION_FIELD_NUMBER: builtins.int
    XLA_DUMP_HLO_SNAPSHOTS_FIELD_NUMBER: builtins.int
    XLA_DUMP_INCLUDE_TIMESTAMP_FIELD_NUMBER: builtins.int
    XLA_DUMP_MAX_HLO_MODULES_FIELD_NUMBER: builtins.int
    XLA_DUMP_MODULE_METADATA_FIELD_NUMBER: builtins.int
    XLA_DUMP_COMPRESS_PROTOS_FIELD_NUMBER: builtins.int
    XLA_DUMP_HLO_AS_LONG_TEXT_FIELD_NUMBER: builtins.int
    XLA_GPU_FORCE_CONV_NCHW_FIELD_NUMBER: builtins.int
    XLA_GPU_FORCE_CONV_NHWC_FIELD_NUMBER: builtins.int
    XLA_GPU_PTX_FILE_FIELD_NUMBER: builtins.int
    XLA_GPU_DUMP_LLVMIR_FIELD_NUMBER: builtins.int
    XLA_GPU_ALGORITHM_DENYLIST_PATH_FIELD_NUMBER: builtins.int
    XLA_TPU_DETECT_NAN_FIELD_NUMBER: builtins.int
    XLA_TPU_DETECT_INF_FIELD_NUMBER: builtins.int
    XLA_CPU_ENABLE_XPROF_TRACEME_FIELD_NUMBER: builtins.int
    XLA_GPU_UNSAFE_FALLBACK_TO_DRIVER_ON_PTXAS_NOT_FOUND_FIELD_NUMBER: builtins.int
    XLA_GPU_ASM_EXTRA_FLAGS_FIELD_NUMBER: builtins.int
    XLA_MULTIHEAP_SIZE_CONSTRAINT_PER_HEAP_FIELD_NUMBER: builtins.int
    XLA_DETAILED_LOGGING_AND_DUMPING_FIELD_NUMBER: builtins.int
    XLA_GPU_FORCE_COMPILATION_PARALLELISM_FIELD_NUMBER: builtins.int
    XLA_GPU_DETERMINISTIC_OPS_FIELD_NUMBER: builtins.int
    XLA_GPU_LLVM_IR_FILE_FIELD_NUMBER: builtins.int
    XLA_GPU_ENABLE_ASYNC_ALL_REDUCE_FIELD_NUMBER: builtins.int
    XLA_GPU_ALL_REDUCE_COMBINE_THRESHOLD_BYTES_FIELD_NUMBER: builtins.int
    XLA_GPU_ALL_REDUCE_CONTIGUOUS_FIELD_NUMBER: builtins.int
    XLA_GPU_ALL_REDUCE_BLUECONNECT_NUM_DEVICES_PER_HOST_FIELD_NUMBER: builtins.int
    XLA_GPU_ENABLE_CUDNN_FRONTEND_FIELD_NUMBER: builtins.int
    XLA_DUMP_DISABLE_METADATA_FIELD_NUMBER: builtins.int
    XLA_DUMP_HLO_PIPELINE_RE_FIELD_NUMBER: builtins.int
    XLA_GPU_STRICT_CONV_ALGORITHM_PICKER_FIELD_NUMBER: builtins.int
    XLA_GPU_ENABLE_XLA_RUNTIME_EXECUTABLE_FIELD_NUMBER: builtins.int
    XLA_GPU_NCCL_TERMINATION_TIMEOUT_SECONDS_FIELD_NUMBER: builtins.int
    XLA_GPU_ENABLE_SHARED_CONSTANTS_FIELD_NUMBER: builtins.int
    XLA_GPU_ENABLE_CUBLASLT_FIELD_NUMBER: builtins.int
    XLA_GPU_REDZONE_SCRATCH_MAX_MEGABYTES_FIELD_NUMBER: builtins.int
    XLA_GPU_SIMPLIFY_ALL_FP_CONVERSIONS_FIELD_NUMBER: builtins.int
    XLA_GPU_NORMALIZE_LAYOUTS_FIELD_NUMBER: builtins.int
    XLA_CPU_USE_ACL_FIELD_NUMBER: builtins.int
    XLA_CPU_STRICT_DOT_CONV_MATH_FIELD_NUMBER: builtins.int
    XLA_BACKEND_EXTRA_OPTIONS_FIELD_NUMBER: builtins.int
    xla_hlo_graph_addresses: builtins.bool
    """Show addresses of HLO ops in graph dump."""
    xla_hlo_profile: builtins.bool
    """Instrument the computation to collect per-HLO cycle counts."""
    @property
    def xla_disable_hlo_passes(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[builtins.str]:
        """List of HLO passes to disable/enable. These names must exactly match the
        pass names as specified by the HloPassInterface::name() method.

        At least one of xla_disable_hlo_passes and xla_enable_hlo_passes_only must
        be empty.
        """
    @property
    def xla_enable_hlo_passes_only(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[builtins.str]: ...
    xla_disable_all_hlo_passes: builtins.bool
    """Disables all HLO passes.  Notes that some passes are necessary for
    correctness and the invariants that must be satisfied by "fully optimized"
    HLO are different for different devices and may change over time.  The only
    "guarantee", such as it is, is that if you compile XLA and dump the
    optimized HLO for some graph, you should be able to run it again on the
    same device with the same build of XLA.
    """
    xla_backend_optimization_level: builtins.int
    """Numerical optimization level for the XLA compiler backend; the specific
    interpretation of this value is left to the backends.
    """
    xla_embed_ir_in_executable: builtins.bool
    """Embed the compiler IR as a string in the executable."""
    xla_eliminate_hlo_implicit_broadcast: builtins.bool
    """Eliminate implicit broadcasts when lowering user computations to HLO
    instructions; use explicit broadcast instead.
    """
    xla_cpu_multi_thread_eigen: builtins.bool
    """When generating calls to Eigen in the CPU backend, use multi-threaded Eigen
    mode.
    """
    xla_gpu_cuda_data_dir: builtins.str
    """Path to directory with cuda/ptx tools and libraries."""
    xla_gpu_ftz: builtins.bool
    """Enable flush-to-zero semantics in the GPU backend."""
    xla_llvm_enable_alias_scope_metadata: builtins.bool
    """If true, in LLVM-based backends, emit !alias.scope metadata in
    generated IR.
    """
    xla_llvm_enable_noalias_metadata: builtins.bool
    """If true, in LLVM-based backends, emit !noalias metadata in the
    generated IR.
    """
    xla_llvm_enable_invariant_load_metadata: builtins.bool
    """If true, in LLVM-based backends, emit !invariant.load metadata in
    the generated IR.
    """
    xla_llvm_disable_expensive_passes: builtins.bool
    """If true, a set of expensive LLVM optimization passes will not be run."""
    xla_test_all_output_layouts: builtins.bool
    """This is used by ClientLibraryTestBase::ComputeAndCompare*. If true, the
    computation will run n! times with all permunations of layouts for the
    output shape in rank n. For example, with a 3D shape, all permutations of
    the set {0, 1, 2} are tried.
    """
    xla_test_all_input_layouts: builtins.bool
    """This is used by ClientLibraryTestBase::ComputeAndCompare*. If true, the
    computation will run for all permunations of layouts of all input
    arguments. For example, with 2 input arguments in 2D and 4D shapes, the
    computation will run 2! * 4! times.
    """
    xla_hlo_graph_sharding_color: builtins.bool
    """Assign colors based on sharding information when generating the Graphviz
    HLO graph.
    """
    xla_cpu_use_mkl_dnn: builtins.bool
    """Generate calls to MKL-DNN in the CPU backend."""
    xla_cpu_use_xla_runtime: builtins.bool
    """Enable XLA Runtime in the CPU backend."""
    xla_gpu_max_kernel_unroll_factor: builtins.int
    """Maximum kernel unroll factor for the GPU backend."""
    xla_cpu_enable_fast_math: builtins.bool
    """When true, "unsafe" mathematical optimizations are enabled. These
    transformations include but are not limited to:

     - Reducing the precision of operations (e.g. using an approximate sin
       function, or transforming x/y into x * (1/y)).
     - Assuming that operations never produce or consume NaN or +/- Inf (this
       behavior can be adjusted using xla_cpu_fast_math_allow_{nans|infs}).
     - Assuming that +0 and -0 are indistinguishable.
    """
    xla_cpu_fast_math_honor_nans: builtins.bool
    """When xla_cpu_enable_fast_math is true then this controls whether we allow
    operations to produce NaNs.  Ignored when xla_cpu_enable_fast_math is
    false.
    """
    xla_cpu_fast_math_honor_infs: builtins.bool
    """When xla_cpu_enable_fast_math is true then this controls whether we allow
    operations to produce infinites. Ignored when xla_cpu_enable_fast_math is
    false.
    """
    xla_cpu_fast_math_honor_division: builtins.bool
    """When xla_cpu_enable_fast_math is true then this controls whether we forbid
    to use the reciprocal of an argument instead of division. Ignored when
    xla_cpu_enable_fast_math is false.
    """
    xla_cpu_fast_math_honor_functions: builtins.bool
    """When xla_cpu_enable_fast_math is true then this controls whether we forbid
    to approximate calculations for functions. Ignored when
    xla_cpu_enable_fast_math is false.
    """
    xla_cpu_enable_fast_min_max: builtins.bool
    """When false we lower the Minimum and Maximum hlos in the CPU backend such
    that Min(NotNaN, NaN) = Min(NaN, NotNaN) = NaN.  In other words, if flag
    this is false we always propagate NaNs through Min and Max.

    Note, this does not correspond to the exact same behavior as the gpu flag
    below!
    """
    xla_gpu_enable_fast_min_max: builtins.bool
    """When true we lower the Minimum and Maximum hlos in the GPU backend such
    that Min(NotNaN, NaN) = Min(NaN, NotNaN) = NotNaN.  In other words, if flag
    this is true we don't propagate NaNs through Min and Max.

    Note, this does not correspond to the exact same behavior as the cpu flag
    above!
    """
    xla_allow_excess_precision: builtins.bool
    """Allows xla to increase the output precision of floating point operations."""
    xla_gpu_crash_on_verification_failures: builtins.bool
    """Crashes the program when any kind of verification fails, instead of just
    logging the failures. One example is cross checking of convolution results
    among different algorithms.
    """
    xla_gpu_autotune_level: builtins.int
    """0:   Disable gemm and convolution autotuning.
    1:   Enable autotuning, but disable correctness checking.
    2:   Also set output buffers to random numbers during autotuning.
    3:   Also reset output buffers to random numbers after autotuning each
         algorithm.
    4+:  Also check for correct outputs and for out-of-bounds reads/writes.

    Default: 4.
    """
    xla_force_host_platform_device_count: builtins.int
    """Force the host platform to pretend that there are these many host
    "devices".  All these devices are backed by the same threadpool.  Defaults
    to 1.

    Setting this to anything other than 1 can increase overhead from context
    switching but we let the user override this behavior to help run tests on
    the host that run models in parallel across multiple devices.
    """
    xla_gpu_disable_gpuasm_optimizations: builtins.bool
    """If set to true XLA:GPU invokes `ptxas` with -O0 (default is -O3)."""
    xla_gpu_shape_checks: global___DebugOptions.ShapeChecks.ValueType
    xla_cpu_enable_mlir_lowering: builtins.bool
    """Enable MLIR-based lowering in XLA:CPU instead of LLVM emitters."""
    xla_gpu_enable_mlir_lowering: builtins.bool
    """If true, use MLIR instead of IR emitter to generate device code for
    supported lmhlo.fusion ops. See xla::gpu::RewriteFusionOps() for details.
    """
    xla_hlo_evaluator_use_fast_path: builtins.bool
    """Enable fast math with eigen in the HLO evaluator."""
    xla_allow_scalar_index_dynamic_ops: builtins.bool
    """Temporary option to allow support for both the R1 and the scalar index
    versions of DynamicSlice and DynamicUpdateSlice. Only used for testing.
    """
    xla_step_marker_location: global___DebugOptions.StepMarkerLocation.ValueType
    """Option to emit a target-specific marker to indicate the start of a training
    step. The location of the marker (if any) is determined by the option
    value.
    """
    xla_dump_to: builtins.str
    """
    BEGIN flags controlling dumping HLO modules for debugging.

    When dumping is enabled, HLO modules dumped at the very beginning and end
    of compilation, and optionally also during the pass pipeline.

    In general, if you set one of these flags, we will try to infer reasonable
    defaults for the others.  For example:

     * Setting --xla_dump_to=/tmp/foo without specifying a format
       with --xla_dump_hlo_as_* will turn on --xla_dump_hlo_as_text.

     * Setting --xla_dump_hlo_as_text without specifying --xla_dump_to will
       dump to stdout.

    Directory to dump into.
    """
    xla_dump_hlo_module_re: builtins.str
    """If specified, will only dump modules which match this regexp."""
    xla_dump_hlo_pass_re: builtins.str
    """If this flag is specified, will also dump HLO before and after passes that
    match this regular expression.  Set to .* to dump before/after all passes.
    """
    xla_dump_hlo_as_text: builtins.bool
    """Specifies the format that HLO is dumped in.  Multiple of these may be
    specified.
    """
    xla_dump_hlo_as_proto: builtins.bool
    xla_dump_hlo_as_dot: builtins.bool
    xla_dump_hlo_as_url: builtins.bool
    xla_dump_hlo_as_html: builtins.bool
    """Dump HLO graphs as an HTML (DOT -> SVG inlined in HTML)"""
    xla_dump_fusion_visualization: builtins.bool
    """Dump the visualization of the fusion progress."""
    xla_dump_hlo_snapshots: builtins.bool
    """If true, every time an HLO module is run, we will dump an HloSnapshot
    (essentially, a serialized module plus its inputs) to the --xla_dump_to
    directory.
    """
    xla_dump_include_timestamp: builtins.bool
    """Include a timestamp in the dumped filenames."""
    xla_dump_max_hlo_modules: builtins.int
    """Max number of hlo module dumps in a directory. Set to < 0 for unbounded."""
    xla_dump_module_metadata: builtins.bool
    """Dump HloModuleMetadata as a text proto for each HLO module."""
    xla_dump_compress_protos: builtins.bool
    """GZip-compress protos dumped via --xla_dump_hlo_as_proto."""
    xla_dump_hlo_as_long_text: builtins.bool
    """Dump HLO in long text format. Ignored unless xla_dump_hlo_as_text is true."""
    xla_gpu_force_conv_nchw: builtins.bool
    """
    END flags controlling dumping HLO modules.

    Overrides for XLA GPU's convolution layout heuristic.
    """
    xla_gpu_force_conv_nhwc: builtins.bool
    @property
    def xla_gpu_ptx_file(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[builtins.str]:
        """Paths to files with ptx code."""
    xla_gpu_dump_llvmir: builtins.bool
    """Whether to dump llvm ir when compiling to ptx."""
    xla_gpu_algorithm_denylist_path: builtins.str
    """Denylist for cuDNN convolutions."""
    xla_tpu_detect_nan: builtins.bool
    """Debug options that trigger execution errors when NaN or Inf are detected."""
    xla_tpu_detect_inf: builtins.bool
    xla_cpu_enable_xprof_traceme: builtins.bool
    """True if TraceMe annotations are enabled for XLA:CPU."""
    xla_gpu_unsafe_fallback_to_driver_on_ptxas_not_found: builtins.bool
    """It is usually preferable to not fallback to the driver; it can consume more
    memory, or have bugs.
    """
    xla_gpu_asm_extra_flags: builtins.str
    """Extra parameters to pass the GPU assembler."""
    xla_multiheap_size_constraint_per_heap: builtins.int
    """Per-heap size constraint. New heaps will be created if per-heap max size is
    reached.
    """
    xla_detailed_logging_and_dumping: builtins.bool
    """Enable detailed logging into vlog and xla dumping. If this is disabled, no
    compilation summary will be printed in the end of computation and no hlo
    modules will be dumped.
    """
    xla_gpu_force_compilation_parallelism: builtins.int
    """Overrides normal multi-threaded compilation settting to use this many
    threads. Setting to 0 (the default value) means no enforcement.
    """
    xla_gpu_deterministic_ops: builtins.bool
    """Guarantees run-to-run determinism. At present, the HLO ops Scatter and
    SelectAndScatter do not have deterministic XLA:GPU implementations.
    Compilation errors out if these ops are encountered.
    """
    @property
    def xla_gpu_llvm_ir_file(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[builtins.str]:
        """Paths to files with LLVM code."""
    xla_gpu_enable_async_all_reduce: builtins.bool
    """Convert synchronous all-reduces ops into asynchronous."""
    xla_gpu_all_reduce_combine_threshold_bytes: builtins.int
    """Size threshold (in bytes) for the GPU all-reduce combiner."""
    xla_gpu_all_reduce_contiguous: builtins.bool
    """Combine GPU all-reduces into a single operation over a contiguous buffer."""
    xla_gpu_all_reduce_blueconnect_num_devices_per_host: builtins.int
    """Number of devices per host for first stage of BlueConnect decomposition
    pass. The pass will attempt to decompose all-reduces ops into a
    ReduceScatter-AllReduce-AllGather sequence, with the initial ReduceScatter
    being performed over all of the devices in the same host. Set to < 1 to
    disable all-reduce decomposition.
    """
    xla_gpu_enable_cudnn_frontend: builtins.bool
    """Whether to use the cuDNN frontend API for convolutions when possible."""
    xla_dump_disable_metadata: builtins.bool
    """Disable dumping metadata in HLO dumps."""
    xla_dump_hlo_pipeline_re: builtins.str
    """If this flag is specified, will only dump HLO before and after passes in
    the pass pipeline that matches this regular expression. Default empty value
    enables dumping in all pipelines.
    """
    xla_gpu_strict_conv_algorithm_picker: builtins.bool
    """If true, abort immediately when conv algorithm picker fails, rather than
    logging a warning and proceeding with fallback.
    """
    xla_gpu_enable_xla_runtime_executable: builtins.bool
    """If true, use XLA runtime for XLA:GPU backend."""
    xla_gpu_nccl_termination_timeout_seconds: builtins.int
    """Timeout in seconds before terminating jobs that are stuck in a NCCL
    Rendezvous. Negative value disables the timeout and will not terminate.
    """
    xla_gpu_enable_shared_constants: builtins.bool
    """Enables shared constants for XLA/GPU. This allows large constants to be
    shared among multiple GPU executables.
    """
    xla_gpu_enable_cublaslt: builtins.bool
    """Whether to use cuBLASLt for GEMMs on GPUs."""
    xla_gpu_redzone_scratch_max_megabytes: builtins.int
    """Size threshold (in megabytes) for the GPU redzone scratch allocator."""
    xla_gpu_simplify_all_fp_conversions: builtins.bool
    """Allows all floating-point conversions to be simplified, including those
    that affect the numerics. The `BFloat16Normalization` pass inserts many
    `f32 -> bf16 -> f32` conversion pairs. These are not removed by the
    `AlgebraicSimplifier`, as that will only simplify conversions that are
    no-ops, e.g. `bf16 -> f32 -> bf16`. Removing these improves accuracy.
    """
    xla_gpu_normalize_layouts: builtins.bool
    """An experimental option to force all layouts present in the
    after-optimizations HLO to be descending, e.g.
    ShapeUtil::MakeShapeWithDescendingLayout is an identity on all
    instructions.
    """
    xla_cpu_use_acl: builtins.bool
    """Generate calls to Arm Compute Library in the CPU backend."""
    xla_cpu_strict_dot_conv_math: builtins.bool
    """By default, XLA:CPU will run fp16 dot/conv as fp32, as this is generally
    (much) faster on our hardware.  Set this flag to disable this behavior.
    """
    @property
    def xla_backend_extra_options(self) -> google.protobuf.internal.containers.ScalarMap[builtins.str, builtins.str]:
        """Next id: 179

        Extra options to pass to the compilation backend (e.g. LLVM); specific
        interpretation of these values is left to the backend.
        """
    def __init__(
        self,
        *,
        xla_hlo_graph_addresses: builtins.bool | None = ...,
        xla_hlo_profile: builtins.bool | None = ...,
        xla_disable_hlo_passes: collections.abc.Iterable[builtins.str] | None = ...,
        xla_enable_hlo_passes_only: collections.abc.Iterable[builtins.str] | None = ...,
        xla_disable_all_hlo_passes: builtins.bool | None = ...,
        xla_backend_optimization_level: builtins.int | None = ...,
        xla_embed_ir_in_executable: builtins.bool | None = ...,
        xla_eliminate_hlo_implicit_broadcast: builtins.bool | None = ...,
        xla_cpu_multi_thread_eigen: builtins.bool | None = ...,
        xla_gpu_cuda_data_dir: builtins.str | None = ...,
        xla_gpu_ftz: builtins.bool | None = ...,
        xla_llvm_enable_alias_scope_metadata: builtins.bool | None = ...,
        xla_llvm_enable_noalias_metadata: builtins.bool | None = ...,
        xla_llvm_enable_invariant_load_metadata: builtins.bool | None = ...,
        xla_llvm_disable_expensive_passes: builtins.bool | None = ...,
        xla_test_all_output_layouts: builtins.bool | None = ...,
        xla_test_all_input_layouts: builtins.bool | None = ...,
        xla_hlo_graph_sharding_color: builtins.bool | None = ...,
        xla_cpu_use_mkl_dnn: builtins.bool | None = ...,
        xla_cpu_use_xla_runtime: builtins.bool | None = ...,
        xla_gpu_max_kernel_unroll_factor: builtins.int | None = ...,
        xla_cpu_enable_fast_math: builtins.bool | None = ...,
        xla_cpu_fast_math_honor_nans: builtins.bool | None = ...,
        xla_cpu_fast_math_honor_infs: builtins.bool | None = ...,
        xla_cpu_fast_math_honor_division: builtins.bool | None = ...,
        xla_cpu_fast_math_honor_functions: builtins.bool | None = ...,
        xla_cpu_enable_fast_min_max: builtins.bool | None = ...,
        xla_gpu_enable_fast_min_max: builtins.bool | None = ...,
        xla_allow_excess_precision: builtins.bool | None = ...,
        xla_gpu_crash_on_verification_failures: builtins.bool | None = ...,
        xla_gpu_autotune_level: builtins.int | None = ...,
        xla_force_host_platform_device_count: builtins.int | None = ...,
        xla_gpu_disable_gpuasm_optimizations: builtins.bool | None = ...,
        xla_gpu_shape_checks: global___DebugOptions.ShapeChecks.ValueType | None = ...,
        xla_cpu_enable_mlir_lowering: builtins.bool | None = ...,
        xla_gpu_enable_mlir_lowering: builtins.bool | None = ...,
        xla_hlo_evaluator_use_fast_path: builtins.bool | None = ...,
        xla_allow_scalar_index_dynamic_ops: builtins.bool | None = ...,
        xla_step_marker_location: global___DebugOptions.StepMarkerLocation.ValueType | None = ...,
        xla_dump_to: builtins.str | None = ...,
        xla_dump_hlo_module_re: builtins.str | None = ...,
        xla_dump_hlo_pass_re: builtins.str | None = ...,
        xla_dump_hlo_as_text: builtins.bool | None = ...,
        xla_dump_hlo_as_proto: builtins.bool | None = ...,
        xla_dump_hlo_as_dot: builtins.bool | None = ...,
        xla_dump_hlo_as_url: builtins.bool | None = ...,
        xla_dump_hlo_as_html: builtins.bool | None = ...,
        xla_dump_fusion_visualization: builtins.bool | None = ...,
        xla_dump_hlo_snapshots: builtins.bool | None = ...,
        xla_dump_include_timestamp: builtins.bool | None = ...,
        xla_dump_max_hlo_modules: builtins.int | None = ...,
        xla_dump_module_metadata: builtins.bool | None = ...,
        xla_dump_compress_protos: builtins.bool | None = ...,
        xla_dump_hlo_as_long_text: builtins.bool | None = ...,
        xla_gpu_force_conv_nchw: builtins.bool | None = ...,
        xla_gpu_force_conv_nhwc: builtins.bool | None = ...,
        xla_gpu_ptx_file: collections.abc.Iterable[builtins.str] | None = ...,
        xla_gpu_dump_llvmir: builtins.bool | None = ...,
        xla_gpu_algorithm_denylist_path: builtins.str | None = ...,
        xla_tpu_detect_nan: builtins.bool | None = ...,
        xla_tpu_detect_inf: builtins.bool | None = ...,
        xla_cpu_enable_xprof_traceme: builtins.bool | None = ...,
        xla_gpu_unsafe_fallback_to_driver_on_ptxas_not_found: builtins.bool | None = ...,
        xla_gpu_asm_extra_flags: builtins.str | None = ...,
        xla_multiheap_size_constraint_per_heap: builtins.int | None = ...,
        xla_detailed_logging_and_dumping: builtins.bool | None = ...,
        xla_gpu_force_compilation_parallelism: builtins.int | None = ...,
        xla_gpu_deterministic_ops: builtins.bool | None = ...,
        xla_gpu_llvm_ir_file: collections.abc.Iterable[builtins.str] | None = ...,
        xla_gpu_enable_async_all_reduce: builtins.bool | None = ...,
        xla_gpu_all_reduce_combine_threshold_bytes: builtins.int | None = ...,
        xla_gpu_all_reduce_contiguous: builtins.bool | None = ...,
        xla_gpu_all_reduce_blueconnect_num_devices_per_host: builtins.int | None = ...,
        xla_gpu_enable_cudnn_frontend: builtins.bool | None = ...,
        xla_dump_disable_metadata: builtins.bool | None = ...,
        xla_dump_hlo_pipeline_re: builtins.str | None = ...,
        xla_gpu_strict_conv_algorithm_picker: builtins.bool | None = ...,
        xla_gpu_enable_xla_runtime_executable: builtins.bool | None = ...,
        xla_gpu_nccl_termination_timeout_seconds: builtins.int | None = ...,
        xla_gpu_enable_shared_constants: builtins.bool | None = ...,
        xla_gpu_enable_cublaslt: builtins.bool | None = ...,
        xla_gpu_redzone_scratch_max_megabytes: builtins.int | None = ...,
        xla_gpu_simplify_all_fp_conversions: builtins.bool | None = ...,
        xla_gpu_normalize_layouts: builtins.bool | None = ...,
        xla_cpu_use_acl: builtins.bool | None = ...,
        xla_cpu_strict_dot_conv_math: builtins.bool | None = ...,
        xla_backend_extra_options: collections.abc.Mapping[builtins.str, builtins.str] | None = ...,
    ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["xla_allow_excess_precision", b"xla_allow_excess_precision", "xla_allow_scalar_index_dynamic_ops", b"xla_allow_scalar_index_dynamic_ops", "xla_backend_extra_options", b"xla_backend_extra_options", "xla_backend_optimization_level", b"xla_backend_optimization_level", "xla_cpu_enable_fast_math", b"xla_cpu_enable_fast_math", "xla_cpu_enable_fast_min_max", b"xla_cpu_enable_fast_min_max", "xla_cpu_enable_mlir_lowering", b"xla_cpu_enable_mlir_lowering", "xla_cpu_enable_xprof_traceme", b"xla_cpu_enable_xprof_traceme", "xla_cpu_fast_math_honor_division", b"xla_cpu_fast_math_honor_division", "xla_cpu_fast_math_honor_functions", b"xla_cpu_fast_math_honor_functions", "xla_cpu_fast_math_honor_infs", b"xla_cpu_fast_math_honor_infs", "xla_cpu_fast_math_honor_nans", b"xla_cpu_fast_math_honor_nans", "xla_cpu_multi_thread_eigen", b"xla_cpu_multi_thread_eigen", "xla_cpu_strict_dot_conv_math", b"xla_cpu_strict_dot_conv_math", "xla_cpu_use_acl", b"xla_cpu_use_acl", "xla_cpu_use_mkl_dnn", b"xla_cpu_use_mkl_dnn", "xla_cpu_use_xla_runtime", b"xla_cpu_use_xla_runtime", "xla_detailed_logging_and_dumping", b"xla_detailed_logging_and_dumping", "xla_disable_all_hlo_passes", b"xla_disable_all_hlo_passes", "xla_disable_hlo_passes", b"xla_disable_hlo_passes", "xla_dump_compress_protos", b"xla_dump_compress_protos", "xla_dump_disable_metadata", b"xla_dump_disable_metadata", "xla_dump_fusion_visualization", b"xla_dump_fusion_visualization", "xla_dump_hlo_as_dot", b"xla_dump_hlo_as_dot", "xla_dump_hlo_as_html", b"xla_dump_hlo_as_html", "xla_dump_hlo_as_long_text", b"xla_dump_hlo_as_long_text", "xla_dump_hlo_as_proto", b"xla_dump_hlo_as_proto", "xla_dump_hlo_as_text", b"xla_dump_hlo_as_text", "xla_dump_hlo_as_url", b"xla_dump_hlo_as_url", "xla_dump_hlo_module_re", b"xla_dump_hlo_module_re", "xla_dump_hlo_pass_re", b"xla_dump_hlo_pass_re", "xla_dump_hlo_pipeline_re", b"xla_dump_hlo_pipeline_re", "xla_dump_hlo_snapshots", b"xla_dump_hlo_snapshots", "xla_dump_include_timestamp", b"xla_dump_include_timestamp", "xla_dump_max_hlo_modules", b"xla_dump_max_hlo_modules", "xla_dump_module_metadata", b"xla_dump_module_metadata", "xla_dump_to", b"xla_dump_to", "xla_eliminate_hlo_implicit_broadcast", b"xla_eliminate_hlo_implicit_broadcast", "xla_embed_ir_in_executable", b"xla_embed_ir_in_executable", "xla_enable_hlo_passes_only", b"xla_enable_hlo_passes_only", "xla_force_host_platform_device_count", b"xla_force_host_platform_device_count", "xla_gpu_algorithm_denylist_path", b"xla_gpu_algorithm_denylist_path", "xla_gpu_all_reduce_blueconnect_num_devices_per_host", b"xla_gpu_all_reduce_blueconnect_num_devices_per_host", "xla_gpu_all_reduce_combine_threshold_bytes", b"xla_gpu_all_reduce_combine_threshold_bytes", "xla_gpu_all_reduce_contiguous", b"xla_gpu_all_reduce_contiguous", "xla_gpu_asm_extra_flags", b"xla_gpu_asm_extra_flags", "xla_gpu_autotune_level", b"xla_gpu_autotune_level", "xla_gpu_crash_on_verification_failures", b"xla_gpu_crash_on_verification_failures", "xla_gpu_cuda_data_dir", b"xla_gpu_cuda_data_dir", "xla_gpu_deterministic_ops", b"xla_gpu_deterministic_ops", "xla_gpu_disable_gpuasm_optimizations", b"xla_gpu_disable_gpuasm_optimizations", "xla_gpu_dump_llvmir", b"xla_gpu_dump_llvmir", "xla_gpu_enable_async_all_reduce", b"xla_gpu_enable_async_all_reduce", "xla_gpu_enable_cublaslt", b"xla_gpu_enable_cublaslt", "xla_gpu_enable_cudnn_frontend", b"xla_gpu_enable_cudnn_frontend", "xla_gpu_enable_fast_min_max", b"xla_gpu_enable_fast_min_max", "xla_gpu_enable_mlir_lowering", b"xla_gpu_enable_mlir_lowering", "xla_gpu_enable_shared_constants", b"xla_gpu_enable_shared_constants", "xla_gpu_enable_xla_runtime_executable", b"xla_gpu_enable_xla_runtime_executable", "xla_gpu_force_compilation_parallelism", b"xla_gpu_force_compilation_parallelism", "xla_gpu_force_conv_nchw", b"xla_gpu_force_conv_nchw", "xla_gpu_force_conv_nhwc", b"xla_gpu_force_conv_nhwc", "xla_gpu_ftz", b"xla_gpu_ftz", "xla_gpu_llvm_ir_file", b"xla_gpu_llvm_ir_file", "xla_gpu_max_kernel_unroll_factor", b"xla_gpu_max_kernel_unroll_factor", "xla_gpu_nccl_termination_timeout_seconds", b"xla_gpu_nccl_termination_timeout_seconds", "xla_gpu_normalize_layouts", b"xla_gpu_normalize_layouts", "xla_gpu_ptx_file", b"xla_gpu_ptx_file", "xla_gpu_redzone_scratch_max_megabytes", b"xla_gpu_redzone_scratch_max_megabytes", "xla_gpu_shape_checks", b"xla_gpu_shape_checks", "xla_gpu_simplify_all_fp_conversions", b"xla_gpu_simplify_all_fp_conversions", "xla_gpu_strict_conv_algorithm_picker", b"xla_gpu_strict_conv_algorithm_picker", "xla_gpu_unsafe_fallback_to_driver_on_ptxas_not_found", b"xla_gpu_unsafe_fallback_to_driver_on_ptxas_not_found", "xla_hlo_evaluator_use_fast_path", b"xla_hlo_evaluator_use_fast_path", "xla_hlo_graph_addresses", b"xla_hlo_graph_addresses", "xla_hlo_graph_sharding_color", b"xla_hlo_graph_sharding_color", "xla_hlo_profile", b"xla_hlo_profile", "xla_llvm_disable_expensive_passes", b"xla_llvm_disable_expensive_passes", "xla_llvm_enable_alias_scope_metadata", b"xla_llvm_enable_alias_scope_metadata", "xla_llvm_enable_invariant_load_metadata", b"xla_llvm_enable_invariant_load_metadata", "xla_llvm_enable_noalias_metadata", b"xla_llvm_enable_noalias_metadata", "xla_multiheap_size_constraint_per_heap", b"xla_multiheap_size_constraint_per_heap", "xla_step_marker_location", b"xla_step_marker_location", "xla_test_all_input_layouts", b"xla_test_all_input_layouts", "xla_test_all_output_layouts", b"xla_test_all_output_layouts", "xla_tpu_detect_inf", b"xla_tpu_detect_inf", "xla_tpu_detect_nan", b"xla_tpu_detect_nan"]) -> None: ...

global___DebugOptions = DebugOptions

@typing_extensions.final
class ExecutionOptions(google.protobuf.message.Message):
    """These settings control how XLA compiles and/or runs code.  Not all settings
    will have an effect on every platform.

    When adding new fields, keep in mind that boolean fields default to false.
    """

    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    SHAPE_WITH_OUTPUT_LAYOUT_FIELD_NUMBER: builtins.int
    SEED_FIELD_NUMBER: builtins.int
    DEBUG_OPTIONS_FIELD_NUMBER: builtins.int
    DEVICE_HANDLES_FIELD_NUMBER: builtins.int
    NUM_REPLICAS_FIELD_NUMBER: builtins.int
    DEVICE_ASSIGNMENT_FIELD_NUMBER: builtins.int
    ALIAS_PASSTHROUGH_PARAMS_FIELD_NUMBER: builtins.int
    NUM_PARTITIONS_FIELD_NUMBER: builtins.int
    LAUNCH_ID_FIELD_NUMBER: builtins.int
    USE_SPMD_PARTITIONING_FIELD_NUMBER: builtins.int
    USE_AUTO_SPMD_PARTITIONING_FIELD_NUMBER: builtins.int
    AUTO_SPMD_PARTITIONING_MESH_SHAPE_FIELD_NUMBER: builtins.int
    AUTO_SPMD_PARTITIONING_MESH_IDS_FIELD_NUMBER: builtins.int
    DEDUPLICATE_HLO_FIELD_NUMBER: builtins.int
    ALLOW_SPMD_SHARDING_PROPAGATION_TO_OUTPUT_FIELD_NUMBER: builtins.int
    @property
    def shape_with_output_layout(self) -> tensorflow.compiler.xla.xla_data_pb2.ShapeProto:
        """This optional field's layout is used as a hint when storing the output of
        this computation.  Subsequent transfers of this output array to the client
        may be faster when using this layout.

        We use a Shape here to accommodate computations that return a tuple.
        """
    seed: builtins.int
    """Used to seed random-number generators used in this computation.  If this is
    0, we generate a seed ourselves.

    TODO(b/32083678): Changing the seed unnecessarily forces a recompilation.
    """
    @property
    def debug_options(self) -> global___DebugOptions: ...
    @property
    def device_handles(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[tensorflow.compiler.xla.xla_data_pb2.DeviceHandle]:
        """This optional field specifies a particular set of devices to run the
        computation on. The computation will be partitioned across these devices.
        If not provided, the default device will be chosen.
        """
    num_replicas: builtins.int
    """Number of replicas of the computation to run. If zero, uses the default
    number of replicas for the XLA service.
    """
    @property
    def device_assignment(self) -> tensorflow.compiler.xla.xla_data_pb2.DeviceAssignmentProto:
        """This optional field specifies the device assignment if known at compile
        time.
        """
    alias_passthrough_params: builtins.bool
    """Alias input and output buffers for parameters that are passed-through XLA
    modules without being changed.
    """
    num_partitions: builtins.int
    """Number of partitions of the computation to run (model parallelism).
    If zero, uses the default number of partitions for the XLA service.
    """
    launch_id: builtins.int
    """Used to identify a set of programs that should be launch together."""
    use_spmd_partitioning: builtins.bool
    """Indicates whether to use SPMD (true) or MPMD (false) partitioning when
    num_partitions > 1 and XLA is requested to partition the input program.
    """
    use_auto_spmd_partitioning: builtins.bool
    """Whether to automatically generate XLA shardings for SPMD partitioner."""
    @property
    def auto_spmd_partitioning_mesh_shape(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[builtins.int]:
        """Device mesh shape used to create the sharding search space when
        use_auto_spmd_partitioning=true.
        """
    @property
    def auto_spmd_partitioning_mesh_ids(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[builtins.int]:
        """Device mesh ids compatible with the above mesh_shape used when
        use_auto_spmd_partitioning=true.
        """
    deduplicate_hlo: builtins.bool
    """If set, deduplicate hlo into function calls to reduce binary size. Only
    works on TPU.
    """
    allow_spmd_sharding_propagation_to_output: builtins.bool
    """Allows sharding propagation to propagate to the outputs. This changes the
    output shape of the computation (which is undesirable), but it can be used
    to allow to run partial compilation to determine what would be the output
    sharding of a computation if XLA would be allowed to propagate the sharding
    which can be used by higher level framework as a way to query intermediate
    sharding of operations when multiple computation would be chained and
    merged together.
    """
    def __init__(
        self,
        *,
        shape_with_output_layout: tensorflow.compiler.xla.xla_data_pb2.ShapeProto | None = ...,
        seed: builtins.int | None = ...,
        debug_options: global___DebugOptions | None = ...,
        device_handles: collections.abc.Iterable[tensorflow.compiler.xla.xla_data_pb2.DeviceHandle] | None = ...,
        num_replicas: builtins.int | None = ...,
        device_assignment: tensorflow.compiler.xla.xla_data_pb2.DeviceAssignmentProto | None = ...,
        alias_passthrough_params: builtins.bool | None = ...,
        num_partitions: builtins.int | None = ...,
        launch_id: builtins.int | None = ...,
        use_spmd_partitioning: builtins.bool | None = ...,
        use_auto_spmd_partitioning: builtins.bool | None = ...,
        auto_spmd_partitioning_mesh_shape: collections.abc.Iterable[builtins.int] | None = ...,
        auto_spmd_partitioning_mesh_ids: collections.abc.Iterable[builtins.int] | None = ...,
        deduplicate_hlo: builtins.bool | None = ...,
        allow_spmd_sharding_propagation_to_output: builtins.bool | None = ...,
    ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["debug_options", b"debug_options", "device_assignment", b"device_assignment", "shape_with_output_layout", b"shape_with_output_layout"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["alias_passthrough_params", b"alias_passthrough_params", "allow_spmd_sharding_propagation_to_output", b"allow_spmd_sharding_propagation_to_output", "auto_spmd_partitioning_mesh_ids", b"auto_spmd_partitioning_mesh_ids", "auto_spmd_partitioning_mesh_shape", b"auto_spmd_partitioning_mesh_shape", "debug_options", b"debug_options", "deduplicate_hlo", b"deduplicate_hlo", "device_assignment", b"device_assignment", "device_handles", b"device_handles", "launch_id", b"launch_id", "num_partitions", b"num_partitions", "num_replicas", b"num_replicas", "seed", b"seed", "shape_with_output_layout", b"shape_with_output_layout", "use_auto_spmd_partitioning", b"use_auto_spmd_partitioning", "use_spmd_partitioning", b"use_spmd_partitioning"]) -> None: ...

global___ExecutionOptions = ExecutionOptions

@typing_extensions.final
class GetDeviceHandlesRequest(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    DEVICE_COUNT_FIELD_NUMBER: builtins.int
    device_count: builtins.int
    def __init__(
        self,
        *,
        device_count: builtins.int | None = ...,
    ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["device_count", b"device_count"]) -> None: ...

global___GetDeviceHandlesRequest = GetDeviceHandlesRequest

@typing_extensions.final
class GetDeviceHandlesResponse(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    DEVICE_HANDLES_FIELD_NUMBER: builtins.int
    @property
    def device_handles(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[tensorflow.compiler.xla.xla_data_pb2.DeviceHandle]: ...
    def __init__(
        self,
        *,
        device_handles: collections.abc.Iterable[tensorflow.compiler.xla.xla_data_pb2.DeviceHandle] | None = ...,
    ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["device_handles", b"device_handles"]) -> None: ...

global___GetDeviceHandlesResponse = GetDeviceHandlesResponse

@typing_extensions.final
class TransferToClientRequest(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    DATA_FIELD_NUMBER: builtins.int
    SHAPE_WITH_LAYOUT_FIELD_NUMBER: builtins.int
    @property
    def data(self) -> tensorflow.compiler.xla.xla_data_pb2.GlobalDataHandle: ...
    @property
    def shape_with_layout(self) -> tensorflow.compiler.xla.xla_data_pb2.ShapeProto:
        """This optional field directs the service to return the literal in this
        layout. A shape is used to hold the layout to accommodate tuples.
        """
    def __init__(
        self,
        *,
        data: tensorflow.compiler.xla.xla_data_pb2.GlobalDataHandle | None = ...,
        shape_with_layout: tensorflow.compiler.xla.xla_data_pb2.ShapeProto | None = ...,
    ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["data", b"data", "shape_with_layout", b"shape_with_layout"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["data", b"data", "shape_with_layout", b"shape_with_layout"]) -> None: ...

global___TransferToClientRequest = TransferToClientRequest

@typing_extensions.final
class TransferToClientResponse(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    LITERAL_FIELD_NUMBER: builtins.int
    @property
    def literal(self) -> tensorflow.compiler.xla.xla_data_pb2.LiteralProto: ...
    def __init__(
        self,
        *,
        literal: tensorflow.compiler.xla.xla_data_pb2.LiteralProto | None = ...,
    ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["literal", b"literal"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["literal", b"literal"]) -> None: ...

global___TransferToClientResponse = TransferToClientResponse

@typing_extensions.final
class TransferToServerRequest(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    LITERAL_FIELD_NUMBER: builtins.int
    DEVICE_HANDLE_FIELD_NUMBER: builtins.int
    @property
    def literal(self) -> tensorflow.compiler.xla.xla_data_pb2.LiteralProto: ...
    @property
    def device_handle(self) -> tensorflow.compiler.xla.xla_data_pb2.DeviceHandle: ...
    def __init__(
        self,
        *,
        literal: tensorflow.compiler.xla.xla_data_pb2.LiteralProto | None = ...,
        device_handle: tensorflow.compiler.xla.xla_data_pb2.DeviceHandle | None = ...,
    ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["device_handle", b"device_handle", "literal", b"literal"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["device_handle", b"device_handle", "literal", b"literal"]) -> None: ...

global___TransferToServerRequest = TransferToServerRequest

@typing_extensions.final
class TransferToServerResponse(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    DATA_FIELD_NUMBER: builtins.int
    @property
    def data(self) -> tensorflow.compiler.xla.xla_data_pb2.GlobalDataHandle: ...
    def __init__(
        self,
        *,
        data: tensorflow.compiler.xla.xla_data_pb2.GlobalDataHandle | None = ...,
    ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["data", b"data"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["data", b"data"]) -> None: ...

global___TransferToServerResponse = TransferToServerResponse

@typing_extensions.final
class TransferToInfeedRequest(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    LITERAL_FIELD_NUMBER: builtins.int
    REPLICA_ID_FIELD_NUMBER: builtins.int
    DEVICE_HANDLE_FIELD_NUMBER: builtins.int
    @property
    def literal(self) -> tensorflow.compiler.xla.xla_data_pb2.LiteralProto: ...
    replica_id: builtins.int
    @property
    def device_handle(self) -> tensorflow.compiler.xla.xla_data_pb2.DeviceHandle: ...
    def __init__(
        self,
        *,
        literal: tensorflow.compiler.xla.xla_data_pb2.LiteralProto | None = ...,
        replica_id: builtins.int | None = ...,
        device_handle: tensorflow.compiler.xla.xla_data_pb2.DeviceHandle | None = ...,
    ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["device_handle", b"device_handle", "literal", b"literal"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["device_handle", b"device_handle", "literal", b"literal", "replica_id", b"replica_id"]) -> None: ...

global___TransferToInfeedRequest = TransferToInfeedRequest

@typing_extensions.final
class TransferToInfeedResponse(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    def __init__(
        self,
    ) -> None: ...

global___TransferToInfeedResponse = TransferToInfeedResponse

@typing_extensions.final
class TransferFromOutfeedRequest(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    SHAPE_WITH_LAYOUT_FIELD_NUMBER: builtins.int
    REPLICA_ID_FIELD_NUMBER: builtins.int
    DEVICE_HANDLE_FIELD_NUMBER: builtins.int
    @property
    def shape_with_layout(self) -> tensorflow.compiler.xla.xla_data_pb2.ShapeProto:
        """This optional field directs the service to return the literal in this
        layout. A shape is used to hold the layout to accommodate tuples.
        """
    replica_id: builtins.int
    @property
    def device_handle(self) -> tensorflow.compiler.xla.xla_data_pb2.DeviceHandle: ...
    def __init__(
        self,
        *,
        shape_with_layout: tensorflow.compiler.xla.xla_data_pb2.ShapeProto | None = ...,
        replica_id: builtins.int | None = ...,
        device_handle: tensorflow.compiler.xla.xla_data_pb2.DeviceHandle | None = ...,
    ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["device_handle", b"device_handle", "shape_with_layout", b"shape_with_layout"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["device_handle", b"device_handle", "replica_id", b"replica_id", "shape_with_layout", b"shape_with_layout"]) -> None: ...

global___TransferFromOutfeedRequest = TransferFromOutfeedRequest

@typing_extensions.final
class TransferFromOutfeedResponse(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    LITERAL_FIELD_NUMBER: builtins.int
    @property
    def literal(self) -> tensorflow.compiler.xla.xla_data_pb2.LiteralProto: ...
    def __init__(
        self,
        *,
        literal: tensorflow.compiler.xla.xla_data_pb2.LiteralProto | None = ...,
    ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["literal", b"literal"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["literal", b"literal"]) -> None: ...

global___TransferFromOutfeedResponse = TransferFromOutfeedResponse

@typing_extensions.final
class ResetDeviceRequest(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    DEVICE_HANDLE_FIELD_NUMBER: builtins.int
    @property
    def device_handle(self) -> tensorflow.compiler.xla.xla_data_pb2.DeviceHandle: ...
    def __init__(
        self,
        *,
        device_handle: tensorflow.compiler.xla.xla_data_pb2.DeviceHandle | None = ...,
    ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["device_handle", b"device_handle"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["device_handle", b"device_handle"]) -> None: ...

global___ResetDeviceRequest = ResetDeviceRequest

@typing_extensions.final
class ResetDeviceResponse(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    def __init__(
        self,
    ) -> None: ...

global___ResetDeviceResponse = ResetDeviceResponse

@typing_extensions.final
class ComputationGraphStatsRequest(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    COMPUTATION_FIELD_NUMBER: builtins.int
    DEBUG_OPTIONS_FIELD_NUMBER: builtins.int
    @property
    def computation(self) -> tensorflow.compiler.xla.service.hlo_pb2.HloModuleProto: ...
    @property
    def debug_options(self) -> global___DebugOptions: ...
    def __init__(
        self,
        *,
        computation: tensorflow.compiler.xla.service.hlo_pb2.HloModuleProto | None = ...,
        debug_options: global___DebugOptions | None = ...,
    ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["computation", b"computation", "debug_options", b"debug_options"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["computation", b"computation", "debug_options", b"debug_options"]) -> None: ...

global___ComputationGraphStatsRequest = ComputationGraphStatsRequest

@typing_extensions.final
class ComputationStatsResponse(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    STATS_FIELD_NUMBER: builtins.int
    @property
    def stats(self) -> tensorflow.compiler.xla.xla_data_pb2.ComputationStats: ...
    def __init__(
        self,
        *,
        stats: tensorflow.compiler.xla.xla_data_pb2.ComputationStats | None = ...,
    ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["stats", b"stats"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["stats", b"stats"]) -> None: ...

global___ComputationStatsResponse = ComputationStatsResponse

@typing_extensions.final
class CreateChannelHandleRequest(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    CHANNEL_TYPE_FIELD_NUMBER: builtins.int
    channel_type: tensorflow.compiler.xla.xla_data_pb2.ChannelHandle.ChannelType.ValueType
    def __init__(
        self,
        *,
        channel_type: tensorflow.compiler.xla.xla_data_pb2.ChannelHandle.ChannelType.ValueType | None = ...,
    ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["channel_type", b"channel_type"]) -> None: ...

global___CreateChannelHandleRequest = CreateChannelHandleRequest

@typing_extensions.final
class CreateChannelHandleResponse(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    CHANNEL_FIELD_NUMBER: builtins.int
    @property
    def channel(self) -> tensorflow.compiler.xla.xla_data_pb2.ChannelHandle: ...
    def __init__(
        self,
        *,
        channel: tensorflow.compiler.xla.xla_data_pb2.ChannelHandle | None = ...,
    ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["channel", b"channel"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["channel", b"channel"]) -> None: ...

global___CreateChannelHandleResponse = CreateChannelHandleResponse

@typing_extensions.final
class UnregisterRequest(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    DATA_FIELD_NUMBER: builtins.int
    @property
    def data(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[tensorflow.compiler.xla.xla_data_pb2.GlobalDataHandle]: ...
    def __init__(
        self,
        *,
        data: collections.abc.Iterable[tensorflow.compiler.xla.xla_data_pb2.GlobalDataHandle] | None = ...,
    ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["data", b"data"]) -> None: ...

global___UnregisterRequest = UnregisterRequest

@typing_extensions.final
class UnregisterResponse(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    def __init__(
        self,
    ) -> None: ...

global___UnregisterResponse = UnregisterResponse

@typing_extensions.final
class CompileRequest(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    COMPUTATION_FIELD_NUMBER: builtins.int
    EXECUTION_OPTIONS_FIELD_NUMBER: builtins.int
    INPUT_SHAPE_WITH_LAYOUT_FIELD_NUMBER: builtins.int
    @property
    def computation(self) -> tensorflow.compiler.xla.service.hlo_pb2.HloModuleProto:
        """The graph to be compiled."""
    @property
    def execution_options(self) -> global___ExecutionOptions:
        """Options that affect how XLA compiles code to service this request."""
    @property
    def input_shape_with_layout(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[tensorflow.compiler.xla.xla_data_pb2.ShapeProto]:
        """The layouts of the input arguments. If not set, the default layout will be
        used. Although the real arguments are not needed in compilation, the
        layouts of the arguments can affect the compilation.
        """
    def __init__(
        self,
        *,
        computation: tensorflow.compiler.xla.service.hlo_pb2.HloModuleProto | None = ...,
        execution_options: global___ExecutionOptions | None = ...,
        input_shape_with_layout: collections.abc.Iterable[tensorflow.compiler.xla.xla_data_pb2.ShapeProto] | None = ...,
    ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["computation", b"computation", "execution_options", b"execution_options"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["computation", b"computation", "execution_options", b"execution_options", "input_shape_with_layout", b"input_shape_with_layout"]) -> None: ...

global___CompileRequest = CompileRequest

@typing_extensions.final
class CompileResponse(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    HANDLE_FIELD_NUMBER: builtins.int
    @property
    def handle(self) -> tensorflow.compiler.xla.xla_data_pb2.ExecutionHandle:
        """The handle to the executable."""
    def __init__(
        self,
        *,
        handle: tensorflow.compiler.xla.xla_data_pb2.ExecutionHandle | None = ...,
    ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["handle", b"handle"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["handle", b"handle"]) -> None: ...

global___CompileResponse = CompileResponse

@typing_extensions.final
class ExecuteRequest(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    HANDLE_FIELD_NUMBER: builtins.int
    ARGUMENTS_FIELD_NUMBER: builtins.int
    @property
    def handle(self) -> tensorflow.compiler.xla.xla_data_pb2.ExecutionHandle: ...
    @property
    def arguments(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[tensorflow.compiler.xla.xla_data_pb2.GlobalDataHandle]:
        """The shape and layout of the arguments must be the same as the those of the
        executable's parameters.
        """
    def __init__(
        self,
        *,
        handle: tensorflow.compiler.xla.xla_data_pb2.ExecutionHandle | None = ...,
        arguments: collections.abc.Iterable[tensorflow.compiler.xla.xla_data_pb2.GlobalDataHandle] | None = ...,
    ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["handle", b"handle"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["arguments", b"arguments", "handle", b"handle"]) -> None: ...

global___ExecuteRequest = ExecuteRequest

@typing_extensions.final
class ExecuteGraphRequest(google.protobuf.message.Message):
    """TODO(b/118493728): Remove this and ExecuteGraphParallelRequest and replace
    the uses with calls to Compile and Execute.
    """

    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    COMPUTATION_FIELD_NUMBER: builtins.int
    ARGUMENTS_FIELD_NUMBER: builtins.int
    EXECUTION_OPTIONS_FIELD_NUMBER: builtins.int
    @property
    def computation(self) -> tensorflow.compiler.xla.service.hlo_pb2.HloModuleProto: ...
    @property
    def arguments(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[tensorflow.compiler.xla.xla_data_pb2.GlobalDataHandle]: ...
    @property
    def execution_options(self) -> global___ExecutionOptions:
        """Options that affect how XLA compiles and runs code to service this request."""
    def __init__(
        self,
        *,
        computation: tensorflow.compiler.xla.service.hlo_pb2.HloModuleProto | None = ...,
        arguments: collections.abc.Iterable[tensorflow.compiler.xla.xla_data_pb2.GlobalDataHandle] | None = ...,
        execution_options: global___ExecutionOptions | None = ...,
    ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["computation", b"computation", "execution_options", b"execution_options"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["arguments", b"arguments", "computation", b"computation", "execution_options", b"execution_options"]) -> None: ...

global___ExecuteGraphRequest = ExecuteGraphRequest

@typing_extensions.final
class ExecuteGraphParallelRequest(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    REQUESTS_FIELD_NUMBER: builtins.int
    @property
    def requests(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___ExecuteGraphRequest]: ...
    def __init__(
        self,
        *,
        requests: collections.abc.Iterable[global___ExecuteGraphRequest] | None = ...,
    ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["requests", b"requests"]) -> None: ...

global___ExecuteGraphParallelRequest = ExecuteGraphParallelRequest

@typing_extensions.final
class ExecuteResponse(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    OUTPUT_FIELD_NUMBER: builtins.int
    PROFILE_FIELD_NUMBER: builtins.int
    @property
    def output(self) -> tensorflow.compiler.xla.xla_data_pb2.GlobalDataHandle: ...
    @property
    def profile(self) -> tensorflow.compiler.xla.xla_data_pb2.ExecutionProfile: ...
    def __init__(
        self,
        *,
        output: tensorflow.compiler.xla.xla_data_pb2.GlobalDataHandle | None = ...,
        profile: tensorflow.compiler.xla.xla_data_pb2.ExecutionProfile | None = ...,
    ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["output", b"output", "profile", b"profile"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["output", b"output", "profile", b"profile"]) -> None: ...

global___ExecuteResponse = ExecuteResponse

@typing_extensions.final
class ExecuteParallelResponse(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    RESPONSES_FIELD_NUMBER: builtins.int
    @property
    def responses(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___ExecuteResponse]: ...
    def __init__(
        self,
        *,
        responses: collections.abc.Iterable[global___ExecuteResponse] | None = ...,
    ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["responses", b"responses"]) -> None: ...

global___ExecuteParallelResponse = ExecuteParallelResponse

@typing_extensions.final
class WaitForExecutionRequest(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    EXECUTION_FIELD_NUMBER: builtins.int
    @property
    def execution(self) -> tensorflow.compiler.xla.xla_data_pb2.ExecutionHandle: ...
    def __init__(
        self,
        *,
        execution: tensorflow.compiler.xla.xla_data_pb2.ExecutionHandle | None = ...,
    ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["execution", b"execution"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["execution", b"execution"]) -> None: ...

global___WaitForExecutionRequest = WaitForExecutionRequest

@typing_extensions.final
class WaitForExecutionResponse(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    OUTPUT_FIELD_NUMBER: builtins.int
    PROFILE_FIELD_NUMBER: builtins.int
    @property
    def output(self) -> tensorflow.compiler.xla.xla_data_pb2.GlobalDataHandle: ...
    @property
    def profile(self) -> tensorflow.compiler.xla.xla_data_pb2.ExecutionProfile: ...
    def __init__(
        self,
        *,
        output: tensorflow.compiler.xla.xla_data_pb2.GlobalDataHandle | None = ...,
        profile: tensorflow.compiler.xla.xla_data_pb2.ExecutionProfile | None = ...,
    ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["output", b"output", "profile", b"profile"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["output", b"output", "profile", b"profile"]) -> None: ...

global___WaitForExecutionResponse = WaitForExecutionResponse

@typing_extensions.final
class ComputeConstantGraphRequest(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    COMPUTATION_FIELD_NUMBER: builtins.int
    OUTPUT_LAYOUT_FIELD_NUMBER: builtins.int
    @property
    def computation(self) -> tensorflow.compiler.xla.service.hlo_pb2.HloModuleProto: ...
    @property
    def output_layout(self) -> tensorflow.compiler.xla.xla_data_pb2.LayoutProto: ...
    def __init__(
        self,
        *,
        computation: tensorflow.compiler.xla.service.hlo_pb2.HloModuleProto | None = ...,
        output_layout: tensorflow.compiler.xla.xla_data_pb2.LayoutProto | None = ...,
    ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["computation", b"computation", "output_layout", b"output_layout"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["computation", b"computation", "output_layout", b"output_layout"]) -> None: ...

global___ComputeConstantGraphRequest = ComputeConstantGraphRequest

@typing_extensions.final
class ComputeConstantResponse(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    LITERAL_FIELD_NUMBER: builtins.int
    @property
    def literal(self) -> tensorflow.compiler.xla.xla_data_pb2.LiteralProto:
        """A LiteralProto is returned directly for this request."""
    def __init__(
        self,
        *,
        literal: tensorflow.compiler.xla.xla_data_pb2.LiteralProto | None = ...,
    ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["literal", b"literal"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["literal", b"literal"]) -> None: ...

global___ComputeConstantResponse = ComputeConstantResponse

@typing_extensions.final
class DeconstructTupleRequest(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    TUPLE_HANDLE_FIELD_NUMBER: builtins.int
    @property
    def tuple_handle(self) -> tensorflow.compiler.xla.xla_data_pb2.GlobalDataHandle: ...
    def __init__(
        self,
        *,
        tuple_handle: tensorflow.compiler.xla.xla_data_pb2.GlobalDataHandle | None = ...,
    ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["tuple_handle", b"tuple_handle"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["tuple_handle", b"tuple_handle"]) -> None: ...

global___DeconstructTupleRequest = DeconstructTupleRequest

@typing_extensions.final
class DeconstructTupleResponse(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    ELEMENT_HANDLES_FIELD_NUMBER: builtins.int
    @property
    def element_handles(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[tensorflow.compiler.xla.xla_data_pb2.GlobalDataHandle]: ...
    def __init__(
        self,
        *,
        element_handles: collections.abc.Iterable[tensorflow.compiler.xla.xla_data_pb2.GlobalDataHandle] | None = ...,
    ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["element_handles", b"element_handles"]) -> None: ...

global___DeconstructTupleResponse = DeconstructTupleResponse

@typing_extensions.final
class LoadDataRequest(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    COLUMNIO_TABLET_PATH_FIELD_NUMBER: builtins.int
    COLUMNIO_FIELD_FIELD_NUMBER: builtins.int
    ELEMENT_SHAPE_FIELD_NUMBER: builtins.int
    OFFSET_FIELD_NUMBER: builtins.int
    LIMIT_FIELD_NUMBER: builtins.int
    ZIP_FIELD_NUMBER: builtins.int
    columnio_tablet_path: builtins.str
    """Describes the path of the ColumnIO tablet to load."""
    columnio_field: builtins.str
    """Describes the field to load within the ColumnIO tablet."""
    @property
    def element_shape(self) -> tensorflow.compiler.xla.xla_data_pb2.ShapeProto:
        """Individual element shape, excluding rows."""
    offset: builtins.int
    """Warning: ColumnIO does not support random-access, so use offset with
    caution in performance-critical scenarios.
    """
    limit: builtins.int
    """Maximum number of elements (with shape element_shape) to load."""
    zip: builtins.bool
    """If more than one item is requested (via limit > 1), then this request
    attribute zips together the produced vectors.
    """
    def __init__(
        self,
        *,
        columnio_tablet_path: builtins.str | None = ...,
        columnio_field: builtins.str | None = ...,
        element_shape: tensorflow.compiler.xla.xla_data_pb2.ShapeProto | None = ...,
        offset: builtins.int | None = ...,
        limit: builtins.int | None = ...,
        zip: builtins.bool | None = ...,
    ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["element_shape", b"element_shape"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["columnio_field", b"columnio_field", "columnio_tablet_path", b"columnio_tablet_path", "element_shape", b"element_shape", "limit", b"limit", "offset", b"offset", "zip", b"zip"]) -> None: ...

global___LoadDataRequest = LoadDataRequest

@typing_extensions.final
class LoadDataResponse(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    DATA_FIELD_NUMBER: builtins.int
    DATA_SHAPE_FIELD_NUMBER: builtins.int
    AVAILABLE_ROWS_FIELD_NUMBER: builtins.int
    ROWS_LOADED_FIELD_NUMBER: builtins.int
    NANOSECONDS_FIELD_NUMBER: builtins.int
    @property
    def data(self) -> tensorflow.compiler.xla.xla_data_pb2.GlobalDataHandle: ...
    @property
    def data_shape(self) -> tensorflow.compiler.xla.xla_data_pb2.ShapeProto: ...
    available_rows: builtins.int
    rows_loaded: builtins.int
    nanoseconds: builtins.int
    def __init__(
        self,
        *,
        data: tensorflow.compiler.xla.xla_data_pb2.GlobalDataHandle | None = ...,
        data_shape: tensorflow.compiler.xla.xla_data_pb2.ShapeProto | None = ...,
        available_rows: builtins.int | None = ...,
        rows_loaded: builtins.int | None = ...,
        nanoseconds: builtins.int | None = ...,
    ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["data", b"data", "data_shape", b"data_shape"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["available_rows", b"available_rows", "data", b"data", "data_shape", b"data_shape", "nanoseconds", b"nanoseconds", "rows_loaded", b"rows_loaded"]) -> None: ...

global___LoadDataResponse = LoadDataResponse

@typing_extensions.final
class GetShapeRequest(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    DATA_FIELD_NUMBER: builtins.int
    @property
    def data(self) -> tensorflow.compiler.xla.xla_data_pb2.GlobalDataHandle: ...
    def __init__(
        self,
        *,
        data: tensorflow.compiler.xla.xla_data_pb2.GlobalDataHandle | None = ...,
    ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["data", b"data"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["data", b"data"]) -> None: ...

global___GetShapeRequest = GetShapeRequest

@typing_extensions.final
class GetShapeResponse(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    SHAPE_FIELD_NUMBER: builtins.int
    @property
    def shape(self) -> tensorflow.compiler.xla.xla_data_pb2.ShapeProto: ...
    def __init__(
        self,
        *,
        shape: tensorflow.compiler.xla.xla_data_pb2.ShapeProto | None = ...,
    ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["shape", b"shape"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["shape", b"shape"]) -> None: ...

global___GetShapeResponse = GetShapeResponse

@typing_extensions.final
class UnpackRequest(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    DATA_FIELD_NUMBER: builtins.int
    @property
    def data(self) -> tensorflow.compiler.xla.xla_data_pb2.GlobalDataHandle: ...
    def __init__(
        self,
        *,
        data: tensorflow.compiler.xla.xla_data_pb2.GlobalDataHandle | None = ...,
    ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["data", b"data"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["data", b"data"]) -> None: ...

global___UnpackRequest = UnpackRequest

@typing_extensions.final
class UnpackResponse(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    TIED_DATA_FIELD_NUMBER: builtins.int
    @property
    def tied_data(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[tensorflow.compiler.xla.xla_data_pb2.GlobalDataHandle]: ...
    def __init__(
        self,
        *,
        tied_data: collections.abc.Iterable[tensorflow.compiler.xla.xla_data_pb2.GlobalDataHandle] | None = ...,
    ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["tied_data", b"tied_data"]) -> None: ...

global___UnpackResponse = UnpackResponse
