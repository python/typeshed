from pygments.util import Future
from typing import Any, Optional

class LexerMeta(type):
    def __new__(mcs: Any, name: Any, bases: Any, d: Any): ...

class Lexer(metaclass=LexerMeta):
    name: Any = ...
    aliases: Any = ...
    filenames: Any = ...
    alias_filenames: Any = ...
    mimetypes: Any = ...
    priority: int = ...
    options: Any = ...
    stripnl: Any = ...
    stripall: Any = ...
    ensurenl: Any = ...
    tabsize: Any = ...
    encoding: Any = ...
    filters: Any = ...
    def __init__(self, **options: Any) -> None: ...
    def add_filter(self, filter_: Any, **options: Any) -> None: ...
    def analyse_text(text: Any) -> None: ...
    def get_tokens(self, text: Any, unfiltered: bool = ...): ...
    def get_tokens_unprocessed(self, text: Any) -> None: ...

class DelegatingLexer(Lexer):
    root_lexer: Any = ...
    language_lexer: Any = ...
    needle: Any = ...
    def __init__(self, _root_lexer: Any, _language_lexer: Any, _needle: Any = ..., **options: Any) -> None: ...
    def get_tokens_unprocessed(self, text: Any): ...

class include(str): ...
class _inherit: ...

inherit: Any

class combined(tuple):
    def __new__(cls, *args: Any): ...
    def __init__(self, *args: Any) -> None: ...

class _PseudoMatch:
    def __init__(self, start: Any, text: Any) -> None: ...
    def start(self, arg: Optional[Any] = ...): ...
    def end(self, arg: Optional[Any] = ...): ...
    def group(self, arg: Optional[Any] = ...): ...
    def groups(self): ...
    def groupdict(self): ...

def bygroups(*args: Any): ...

class _This: ...

this: Any

def using(_other: Any, **kwargs: Any): ...

class default:
    state: Any = ...
    def __init__(self, state: Any) -> None: ...

class words(Future):
    words: Any = ...
    prefix: Any = ...
    suffix: Any = ...
    def __init__(self, words: Any, prefix: str = ..., suffix: str = ...) -> None: ...
    def get(self): ...

class RegexLexerMeta(LexerMeta):
    def process_tokendef(cls, name: Any, tokendefs: Optional[Any] = ...): ...
    def get_tokendefs(cls): ...
    def __call__(cls, *args: Any, **kwds: Any): ...

class RegexLexer(Lexer, metaclass=RegexLexerMeta):
    flags: Any = ...
    tokens: Any = ...
    def get_tokens_unprocessed(self, text: Any, stack: Any = ...) -> None: ...

class LexerContext:
    text: Any = ...
    pos: Any = ...
    end: Any = ...
    stack: Any = ...
    def __init__(self, text: Any, pos: Any, stack: Optional[Any] = ..., end: Optional[Any] = ...) -> None: ...

class ExtendedRegexLexer(RegexLexer):
    def get_tokens_unprocessed(self, text: Optional[Any] = ..., context: Optional[Any] = ...) -> None: ...

class ProfilingRegexLexerMeta(RegexLexerMeta): ...

class ProfilingRegexLexer(RegexLexer, metaclass=ProfilingRegexLexerMeta):
    def get_tokens_unprocessed(self, text: Any, stack: Any = ...): ...
