from pygments.util import Future
from typing import Any, Optional

class LexerMeta(type):
    def __new__(mcs, name, bases, d): ...

class Lexer(metaclass=LexerMeta):
    name: Any
    aliases: Any
    filenames: Any
    alias_filenames: Any
    mimetypes: Any
    priority: int
    options: Any
    stripnl: Any
    stripall: Any
    ensurenl: Any
    tabsize: Any
    encoding: Any
    filters: Any
    def __init__(self, **options) -> None: ...
    def add_filter(self, filter_, **options) -> None: ...
    def analyse_text(text) -> None: ...
    def get_tokens(self, text, unfiltered: bool = ...): ...
    def get_tokens_unprocessed(self, text) -> None: ...

class DelegatingLexer(Lexer):
    root_lexer: Any
    language_lexer: Any
    needle: Any
    def __init__(self, _root_lexer, _language_lexer, _needle = ..., **options) -> None: ...
    def get_tokens_unprocessed(self, text): ...

class include(str): ...
class _inherit: ...

inherit: Any

class combined(tuple):
    def __new__(cls, *args): ...
    def __init__(self, *args) -> None: ...

class _PseudoMatch:
    def __init__(self, start, text) -> None: ...
    def start(self, arg: Optional[Any] = ...): ...
    def end(self, arg: Optional[Any] = ...): ...
    def group(self, arg: Optional[Any] = ...): ...
    def groups(self): ...
    def groupdict(self): ...

def bygroups(*args): ...

class _This: ...

this: Any

def using(_other, **kwargs): ...

class default:
    state: Any
    def __init__(self, state) -> None: ...

class words(Future):
    words: Any
    prefix: Any
    suffix: Any
    def __init__(self, words, prefix: str = ..., suffix: str = ...) -> None: ...
    def get(self): ...

class RegexLexerMeta(LexerMeta):
    def process_tokendef(cls, name, tokendefs: Optional[Any] = ...): ...
    def get_tokendefs(cls): ...
    def __call__(cls, *args, **kwds): ...

class RegexLexer(Lexer, metaclass=RegexLexerMeta):
    flags: Any
    tokens: Any
    def get_tokens_unprocessed(self, text, stack = ...) -> None: ...

class LexerContext:
    text: Any
    pos: Any
    end: Any
    stack: Any
    def __init__(self, text, pos, stack: Optional[Any] = ..., end: Optional[Any] = ...) -> None: ...

class ExtendedRegexLexer(RegexLexer):
    def get_tokens_unprocessed(self, text: Optional[Any] = ..., context: Optional[Any] = ...) -> None: ...

class ProfilingRegexLexerMeta(RegexLexerMeta): ...

class ProfilingRegexLexer(RegexLexer, metaclass=ProfilingRegexLexerMeta):
    def get_tokens_unprocessed(self, text, stack = ...): ...
