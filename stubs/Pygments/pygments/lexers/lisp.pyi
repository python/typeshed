from pygments.lexer import RegexLexer
from typing import Any

class SchemeLexer(RegexLexer):
    name: str = ...
    aliases: Any = ...
    filenames: Any = ...
    mimetypes: Any = ...
    keywords: Any = ...
    builtins: Any = ...
    valid_name: str = ...
    tokens: Any = ...

class CommonLispLexer(RegexLexer):
    name: str = ...
    aliases: Any = ...
    filenames: Any = ...
    mimetypes: Any = ...
    flags: Any = ...
    nonmacro: str = ...
    constituent: Any = ...
    terminated: str = ...
    symbol: Any = ...
    builtin_function: Any = ...
    special_forms: Any = ...
    macros: Any = ...
    lambda_list_keywords: Any = ...
    declarations: Any = ...
    builtin_types: Any = ...
    builtin_classes: Any = ...
    def __init__(self, **options: Any) -> None: ...
    def get_tokens_unprocessed(self, text: Any) -> None: ...
    tokens: Any = ...

class HyLexer(RegexLexer):
    name: str = ...
    aliases: Any = ...
    filenames: Any = ...
    mimetypes: Any = ...
    special_forms: Any = ...
    declarations: Any = ...
    hy_builtins: Any = ...
    hy_core: Any = ...
    builtins: Any = ...
    valid_name: str = ...
    tokens: Any = ...
    def analyse_text(text: Any): ...

class RacketLexer(RegexLexer):
    name: str = ...
    aliases: Any = ...
    filenames: Any = ...
    mimetypes: Any = ...
    tokens: Any = ...

class NewLispLexer(RegexLexer):
    name: str = ...
    aliases: Any = ...
    filenames: Any = ...
    mimetypes: Any = ...
    flags: Any = ...
    builtins: Any = ...
    valid_name: str = ...
    tokens: Any = ...

class EmacsLispLexer(RegexLexer):
    name: str = ...
    aliases: Any = ...
    filenames: Any = ...
    mimetypes: Any = ...
    flags: Any = ...
    nonmacro: str = ...
    constituent: Any = ...
    terminated: str = ...
    symbol: Any = ...
    macros: Any = ...
    special_forms: Any = ...
    builtin_function: Any = ...
    builtin_function_highlighted: Any = ...
    lambda_list_keywords: Any = ...
    error_keywords: Any = ...
    def get_tokens_unprocessed(self, text: Any) -> None: ...
    tokens: Any = ...

class ShenLexer(RegexLexer):
    name: str = ...
    aliases: Any = ...
    filenames: Any = ...
    mimetypes: Any = ...
    DECLARATIONS: Any = ...
    SPECIAL_FORMS: Any = ...
    BUILTINS: Any = ...
    BUILTINS_ANYWHERE: Any = ...
    MAPPINGS: Any = ...
    valid_symbol_chars: str = ...
    valid_name: Any = ...
    symbol_name: Any = ...
    variable: Any = ...
    tokens: Any = ...
    def get_tokens_unprocessed(self, text: Any): ...

class CPSALexer(SchemeLexer):
    name: str = ...
    aliases: Any = ...
    filenames: Any = ...
    mimetypes: Any = ...
    valid_name: str = ...
    tokens: Any = ...

class XtlangLexer(RegexLexer):
    name: str = ...
    aliases: Any = ...
    filenames: Any = ...
    mimetypes: Any = ...
    common_keywords: Any = ...
    scheme_keywords: Any = ...
    xtlang_bind_keywords: Any = ...
    xtlang_keywords: Any = ...
    common_functions: Any = ...
    scheme_functions: Any = ...
    xtlang_functions: Any = ...
    valid_scheme_name: str = ...
    valid_xtlang_name: str = ...
    valid_xtlang_type: str = ...
    tokens: Any = ...

class FennelLexer(RegexLexer):
    name: str = ...
    aliases: Any = ...
    filenames: Any = ...
    special_forms: Any = ...
    builtins: Any = ...
    valid_name: str = ...
    tokens: Any = ...
