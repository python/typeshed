from pygments.lexer import ExtendedRegexLexer, RegexLexer
from typing import Any, Optional

class ProtoBufLexer(RegexLexer):
    name: str = ...
    aliases: Any = ...
    filenames: Any = ...
    tokens: Any = ...

class ThriftLexer(RegexLexer):
    name: str = ...
    aliases: Any = ...
    filenames: Any = ...
    mimetypes: Any = ...
    tokens: Any = ...

class ZeekLexer(RegexLexer):
    name: str = ...
    aliases: Any = ...
    filenames: Any = ...
    tokens: Any = ...
BroLexer = ZeekLexer

class PuppetLexer(RegexLexer):
    name: str = ...
    aliases: Any = ...
    filenames: Any = ...
    tokens: Any = ...

class RslLexer(RegexLexer):
    name: str = ...
    aliases: Any = ...
    filenames: Any = ...
    mimetypes: Any = ...
    flags: Any = ...
    tokens: Any = ...
    def analyse_text(text: Any): ...

class MscgenLexer(RegexLexer):
    name: str = ...
    aliases: Any = ...
    filenames: Any = ...
    tokens: Any = ...

class VGLLexer(RegexLexer):
    name: str = ...
    aliases: Any = ...
    filenames: Any = ...
    flags: Any = ...
    tokens: Any = ...

class AlloyLexer(RegexLexer):
    name: str = ...
    aliases: Any = ...
    filenames: Any = ...
    mimetypes: Any = ...
    flags: Any = ...
    iden_rex: str = ...
    text_tuple: Any = ...
    tokens: Any = ...

class PanLexer(RegexLexer):
    name: str = ...
    aliases: Any = ...
    filenames: Any = ...
    tokens: Any = ...

class CrmshLexer(RegexLexer):
    name: str = ...
    aliases: Any = ...
    filenames: Any = ...
    mimetypes: Any = ...
    elem: Any = ...
    sub: Any = ...
    acl: Any = ...
    bin_rel: Any = ...
    un_ops: Any = ...
    date_exp: Any = ...
    acl_mod: str = ...
    bin_ops: str = ...
    val_qual: str = ...
    rsc_role_action: str = ...
    tokens: Any = ...

class FlatlineLexer(RegexLexer):
    name: str = ...
    aliases: Any = ...
    filenames: Any = ...
    mimetypes: Any = ...
    special_forms: Any = ...
    builtins: Any = ...
    valid_name: str = ...
    tokens: Any = ...

class SnowballLexer(ExtendedRegexLexer):
    name: str = ...
    aliases: Any = ...
    filenames: Any = ...
    def __init__(self, **options: Any) -> None: ...
    tokens: Any = ...
    def get_tokens_unprocessed(self, text: Optional[Any] = ..., context: Optional[Any] = ...): ...
