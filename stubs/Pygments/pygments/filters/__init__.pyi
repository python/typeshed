import re
from collections.abc import Callable, Generator, Iterable, Iterator
from typing import Any

from pygments.filter import Filter
from pygments.lexer import Lexer
from pygments.token import _TokenType

def find_filter_class(filtername: str) -> type[Filter]: ...

# options are forwarded to the filter's constructor
def get_filter_by_name(filtername: str, **options: Any) -> Filter: ...
def get_all_filters() -> Generator[str, None, None]: ...

class CodeTagFilter(Filter):
    tag_re: re.Pattern[str]
    # options are forwarded to Filter's constructor
    def __init__(self, **options: Any) -> None: ...
    def filter(self, lexer: Lexer, stream: Iterable[tuple[_TokenType, str]]) -> Iterator[tuple[_TokenType, str]]: ...

class SymbolFilter(Filter):
    latex_symbols: dict[str, str]
    isabelle_symbols: dict[str, str]
    lang_map: dict[str, dict[str, str]]
    symbols: dict[str, str]
    # options are forwarded to Filter's constructor
    def __init__(self, **options: Any) -> None: ...
    def filter(self, lexer: Lexer, stream: Iterable[tuple[_TokenType, str]]) -> Iterator[tuple[_TokenType, str]]: ...

class KeywordCaseFilter(Filter):
    convert: Callable[[str], str]
    # options are forwarded to Filter's constructor
    def __init__(self, **options: Any) -> None: ...
    def filter(self, lexer: Lexer, stream: Iterable[tuple[_TokenType, str]]) -> Iterator[tuple[_TokenType, str]]: ...

class NameHighlightFilter(Filter):
    names: set[str]
    tokentype: _TokenType
    # options are forwarded to Filter's constructor
    def __init__(self, **options: Any) -> None: ...
    def filter(self, lexer: Lexer, stream: Iterable[tuple[_TokenType, str]]) -> Iterator[tuple[_TokenType, str]]: ...

class ErrorToken(Exception): ...

class RaiseOnErrorTokenFilter(Filter):
    exception: type[Exception]
    # options are forwarded to Filter's constructor
    def __init__(self, **options: Any) -> None: ...
    def filter(self, lexer: Lexer, stream: Iterable[tuple[_TokenType, str]]) -> Iterator[tuple[_TokenType, str]]: ...

class VisibleWhitespaceFilter(Filter):
    wstt: bool
    # options are forwarded to Filter's constructor
    def __init__(self, **options: Any) -> None: ...
    def filter(self, lexer: Lexer, stream: Iterable[tuple[_TokenType, str]]) -> Iterator[tuple[_TokenType, str]]: ...

class GobbleFilter(Filter):
    n: int
    # options are forwarded to Filter's constructor
    def __init__(self, **options: Any) -> None: ...
    def gobble(self, value: str, left: int) -> tuple[str, int]: ...
    def filter(self, lexer: Lexer, stream: Iterable[tuple[_TokenType, str]]) -> Iterator[tuple[_TokenType, str]]: ...

class TokenMergeFilter(Filter):
    # options are forwarded to Filter's constructor
    def __init__(self, **options: Any) -> None: ...
    def filter(self, lexer: Lexer, stream: Iterable[tuple[_TokenType, str]]) -> Iterator[tuple[_TokenType, str]]: ...

FILTERS: dict[str, type[Filter]]
